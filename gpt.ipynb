{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "from collections import deque\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "class DQN(nn.Module):\n",
    "    def __init__(self, state_size, action_size, n_assets):\n",
    "        super(DQN, self).__init__()\n",
    "        self.fc1 = nn.Linear(state_size, 24)\n",
    "        self.fc2 = nn.Linear(24, 24)\n",
    "        self.fc3 = nn.Linear(24, 24)\n",
    "\n",
    "        self.logits_layer =  nn.ModuleList()\n",
    "        self.softmax = nn.Softmax()\n",
    "\n",
    "        for i in range(n_assets):\n",
    "            self.logits_layer.append(nn.Linear(24, action_size))\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = torch.relu(self.fc3(x))\n",
    "        outputs = []\n",
    "        for i in range(len(self.logits_layer)):\n",
    "            outputs.append(self.softmax(self.logits_layer[i](x)))\n",
    "        return torch.stack(outputs)\n",
    "\n",
    "class DQNAgent:\n",
    "    def __init__(self, state_size, action_size = 3, n_assets = 2):\n",
    "        \n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.memory = deque(maxlen=2000)\n",
    "        self.gamma = 0.95    # Discount rate\n",
    "        self.epsilon = 1.0   # Exploration rate\n",
    "        self.epsilon_min = 0.01\n",
    "        self.epsilon_decay = 0.995\n",
    "\n",
    "        self.network = DQN(state_size, action_size, n_assets)\n",
    "        self.target_network = DQN(state_size, action_size, n_assets)\n",
    "    \n",
    "        self.criterion = nn.MSELoss()\n",
    "        self.optimizer = optim.Adam(self.network.parameters(), lr=0.001)\n",
    "        self.n_assets = n_assets\n",
    "\n",
    "    def remember(self, state, action, reward, next_state, done):\n",
    "        self.memory.append((state, action, reward, next_state, done))\n",
    "\n",
    "    def act(self, state):\n",
    "        actions = []\n",
    "        state = torch.from_numpy(state[2]).float().unsqueeze(0)\n",
    "        q_values = self.network(state)\n",
    "        for i in range(self.n_assets):\n",
    "            if np.random.rand() <= self.epsilon:\n",
    "                actions.append(random.randrange(self.action_size))\n",
    "            else:\n",
    "                actions.append(q_values[i].max(1)[1].item())\n",
    "        return actions \n",
    "\n",
    "    def replay(self, batch_size = 10):\n",
    "        if len(self.memory) < batch_size:\n",
    "            return\n",
    "        \n",
    "        minibatch = random.sample(self.memory, batch_size)\n",
    "        for state, action, reward, next_state, done in minibatch:\n",
    "            \n",
    "            state = torch.from_numpy(state[2]).float().unsqueeze(0)\n",
    "            next_state = torch.from_numpy(next_state[2]).float().unsqueeze(0)\n",
    "            q_expected = self.network(state)\n",
    "            q_expected_target = q_expected.clone().detach()\n",
    "            q_next = self.network(next_state)       \n",
    "            \n",
    "            for i, a in enumerate(action):\n",
    "                q_target = reward\n",
    "                if not done:\n",
    "                    q_target = (reward + self.gamma * q_next[i].detach().max())\n",
    "                \n",
    "                q_expected_target[i][0][a] = q_target\n",
    "            \n",
    "            loss = self.criterion(q_expected, q_expected_target)\n",
    "            self.optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "\n",
    "        if self.epsilon > self.epsilon_min:\n",
    "            self.epsilon *= self.epsilon_decay\n",
    "\n",
    "# Define your environment and data\n",
    "# You'll need to replace these placeholders with your actual environment and data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "/tmp/ipykernel_62538/725657504.py:28: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  outputs.append(self.softmax(self.logits_layer[i](x)))\n"
     ]
    }
   ],
   "source": [
    "env = TradingEnvironment(tickers = [\"BTC-USD\", \"ETH-USD\"], start_date=\"2012-01-01\", end_date=\"2021-05-01\")\n",
    "\n",
    "\n",
    "state_size = 2  # Example state size\n",
    "action_size = 3  # Example action size\n",
    "n_assets = 2\n",
    "agent = DQNAgent(state_size, action_size, n_assets)\n",
    "EPISODES = 200\n",
    "# Training loop\n",
    "# Replace this with your actual training loop using your data\n",
    "for episode in range(EPISODES):\n",
    "    state = env.reset()\n",
    "    for time in range(500):\n",
    "        action = agent.act(state)\n",
    "        next_state, reward, done = env.step(action)\n",
    "        reward = reward if not done else -10\n",
    "        agent.remember(state, action, reward, next_state, done)\n",
    "        state = next_state\n",
    "        if done:\n",
    "            print(\"episode: {}/{}, running capital: {}, e: {:.2}\"\n",
    "                  .format(episode, EPISODES, env.balance + env.current_price * env.position, agent.epsilon))\n",
    "            break\n",
    "    agent.replay()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_62538/2442818340.py:28: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  outputs.append(self.softmax(self.logits_layer[i](x)))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[0.3924, 0.2729, 0.3347],\n",
       "        [0.2977, 0.2972, 0.4051]], grad_fn=<StackBackward0>)"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dqn = DQN(state_size=2,action_size=3,n_assets=2)\n",
    "dqn(torch.tensor([2.0,5.0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_62538/755782554.py:28: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  outputs.append(self.softmax(self.logits_layer[i](x)))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1, 0]"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent = DQNAgent(state_size=2)\n",
    "agent.act((_, _, np.array([2.0,5.0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yfinance as yf\n",
    "import numpy as np\n",
    "\n",
    "class TradingEnvironment:\n",
    "    def __init__(self, tickers, start_date, end_date):\n",
    "        self.tickers = tickers\n",
    "        self.start_date = start_date\n",
    "        self.end_date = end_date\n",
    "        self.data = self._load_data()\n",
    "        self.reset()\n",
    "\n",
    "    def _load_data(self):\n",
    "        \n",
    "        stocks_data = []\n",
    "\n",
    "        for ticker in self.tickers:\n",
    "            stocks_data.append(yf.download(ticker, \n",
    "                                 start=self.start_date, \n",
    "                                 end=self.end_date))\n",
    "\n",
    "        return stocks_data\n",
    "\n",
    "    def reset(self):\n",
    "        self.current_step = 0\n",
    "        self.total_steps = len(self.data[0]) - 1\n",
    "        self.balance = 10000  # Initial account balance\n",
    "        self.positions = [0 for _ in self.data]# Initial position\n",
    "        self.current_prices = [self.data[i]['Close'][self.current_step] for i in range(len(self.data))]\n",
    "        self.done = False\n",
    "        return self._get_observation()\n",
    "\n",
    "    def _get_observation(self):\n",
    "        observation = (self.balance, self.positions, np.array(self.current_prices))\n",
    "        return observation\n",
    "\n",
    "    def step(self, actions):\n",
    "        if self.done:\n",
    "            raise ValueError(\"Episode is done, call reset() to start a new episode\")\n",
    "\n",
    "        self.current_step += 1\n",
    "        if self.current_step > self.total_steps:\n",
    "            self.done = True\n",
    "            next_state = None\n",
    "            reward = 0\n",
    "            return next_state, reward, self.done\n",
    "\n",
    "        self.current_prices = [self.data[i]['Close'][self.current_step] for i, _ in enumerate(self.data)]\n",
    "\n",
    "        for i, action in enumerate(actions):\n",
    "            if action == 0:  # Buy\n",
    "                if self.balance >= self.current_prices[i]:\n",
    "                    self.positions[i] += 1\n",
    "                    self.balance -= self.current_prices[i]\n",
    "            elif action == 1:  # Sell\n",
    "                if self.positions[i] > 0:\n",
    "                    self.positions[i] -= 1\n",
    "                    self.balance += self.current_prices[i]\n",
    "            elif action == 2: # Hold\n",
    "                pass\n",
    "\n",
    "        next_state = self._get_observation()\n",
    "\n",
    "        # Calculate reward\n",
    "        reward = 0\n",
    "        for i, _ in enumerate(self.data):\n",
    "            reward += (self.positions[i] * self.current_prices[i])   # Reward based on account balance\n",
    "        reward += self.balance - 10000\n",
    "        return next_state, reward, self.done\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((5901.542892456055, [1, 1], [3943.409423828125, 155.0476837158203]),\n",
       " 0.0,\n",
       " False)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env = TradingEnvironment(tickers = [\"BTC-USD\", \"ETH-USD\"], start_date=\"2019-01-01\", end_date=\"2021-05-01\")\n",
    "env.step([0,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((9703.848777770996, [0, 0], [3632.070556640625, 122.55360412597656]),\n",
       " -296.1512222290039,\n",
       " False)"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.step([2,2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%%**********************]  1 of 1 completed\n"
     ]
    }
   ],
   "source": [
    "env = TradingEnvironment(ticker = \"ETH-USD\", start_date=\"2015-01-01\", end_date=\"2021-05-01\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%%**********************]  1 of 1 completed"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[*********************100%%**********************]  1 of 1 completed"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[2.6474e-04, 9.9973e-01, 2.3970e-06]],\n",
      "\n",
      "        [[0.0000e+00, 1.0000e+00, 0.0000e+00]]])\n",
      "tensor([[[1.3716e+03, 9.9973e-01, 2.3970e-06]],\n",
      "\n",
      "        [[0.0000e+00, 1.0000e+00, 0.0000e+00]]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "/tmp/ipykernel_62538/3334161326.py:28: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  outputs.append(self.softmax(self.logits_layer[i](x)))\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "index 1 is out of bounds for dimension 0 with size 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/home/anonymous/Documents/3A ENSAI/RL/gpt.ipynb Cell 8\u001b[0m line \u001b[0;36m2\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/anonymous/Documents/3A%20ENSAI/RL/gpt.ipynb#W4sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m         \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mepisode: \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m/\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m, running capital: \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m, e: \u001b[39m\u001b[39m{:.2}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/anonymous/Documents/3A%20ENSAI/RL/gpt.ipynb#W4sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m               \u001b[39m.\u001b[39mformat(episode, EPISODES, env\u001b[39m.\u001b[39mbalance \u001b[39m+\u001b[39m env\u001b[39m.\u001b[39mcurrent_price \u001b[39m*\u001b[39m env\u001b[39m.\u001b[39mposition, agent\u001b[39m.\u001b[39mepsilon))\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/anonymous/Documents/3A%20ENSAI/RL/gpt.ipynb#W4sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m         \u001b[39mbreak\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/anonymous/Documents/3A%20ENSAI/RL/gpt.ipynb#W4sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m agent\u001b[39m.\u001b[39;49mreplay()\n",
      "\u001b[1;32m/home/anonymous/Documents/3A ENSAI/RL/gpt.ipynb Cell 8\u001b[0m line \u001b[0;36m8\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/anonymous/Documents/3A%20ENSAI/RL/gpt.ipynb#W4sZmlsZQ%3D%3D?line=78'>79</a>\u001b[0m         q_target \u001b[39m=\u001b[39m (reward \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgamma \u001b[39m*\u001b[39m q_next[i]\u001b[39m.\u001b[39mdetach()\u001b[39m.\u001b[39mmax())\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/anonymous/Documents/3A%20ENSAI/RL/gpt.ipynb#W4sZmlsZQ%3D%3D?line=80'>81</a>\u001b[0m     \u001b[39mprint\u001b[39m(q_expected_target)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/anonymous/Documents/3A%20ENSAI/RL/gpt.ipynb#W4sZmlsZQ%3D%3D?line=81'>82</a>\u001b[0m     q_expected_target[\u001b[39m0\u001b[39;49m][i][a] \u001b[39m=\u001b[39m q_target\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/anonymous/Documents/3A%20ENSAI/RL/gpt.ipynb#W4sZmlsZQ%3D%3D?line=83'>84</a>\u001b[0m loss \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcriterion(q_expected, q_expected_target)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/anonymous/Documents/3A%20ENSAI/RL/gpt.ipynb#W4sZmlsZQ%3D%3D?line=84'>85</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moptimizer\u001b[39m.\u001b[39mzero_grad()\n",
      "\u001b[0;31mIndexError\u001b[0m: index 1 is out of bounds for dimension 0 with size 1"
     ]
    }
   ],
   "source": [
    "env = TradingEnvironment(tickers = [\"BTC-USD\", \"ETH-USD\"], start_date=\"2019-01-01\", end_date=\"2021-05-01\")\n",
    "\n",
    "\n",
    "state_size = 2  # Example state size\n",
    "action_size = 3  # Example action size\n",
    "n_assets = 2\n",
    "agent = DQNAgent(state_size, action_size, n_assets)\n",
    "EPISODES = 50\n",
    "# Training loop\n",
    "# Replace this with your actual training loop using your data\n",
    "for episode in range(EPISODES):\n",
    "    state = env.reset()\n",
    "    for time in range(500):\n",
    "        action = agent.act(state)\n",
    "        next_state, reward, done = env.step(action)\n",
    "        reward = reward if not done else -10\n",
    "        agent.remember(state, action, reward, next_state, done)\n",
    "        state = next_state\n",
    "        if done:\n",
    "            print(\"episode: {}/{}, running capital: {}, e: {:.2}\"\n",
    "                  .format(episode, EPISODES, env.balance + env.current_price * env.position, agent.epsilon))\n",
    "            break\n",
    "    agent.replay()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%%**********************]  1 of 1 completed\n"
     ]
    }
   ],
   "source": [
    "env = TradingEnvironment(ticker = \"ETH-USD\", start_date=\"2021-11-01\", end_date=\"2022-08-01\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9700.747009277344\n",
      "9386.066009521484\n",
      "9078.158020019531\n",
      "8761.442016601562\n",
      "8423.811004638672\n",
      "8757.167999267578\n",
      "8426.24398803711\n",
      "8758.638000488281\n",
      "8411.026000976562\n",
      "8056.6400146484375\n",
      "7689.910003662109\n",
      "7329.509002685547\n",
      "6948.856994628906\n",
      "6538.691009521484\n",
      "6063.779998779297\n",
      "6530.055999755859\n",
      "6530.055999755859\n",
      "7010.4110107421875\n",
      "7153.2340087890625\n",
      "7061.898010253906\n",
      "7509.012023925781\n",
      "7975.552032470703\n",
      "8439.001037597656\n",
      "8904.854034423828\n",
      "9375.058044433594\n",
      "8911.777038574219\n",
      "8483.189025878906\n",
      "7654.008026123047\n",
      "7580.533020019531\n",
      "7107.031005859375\n",
      "6665.309997558594\n",
      "5760.628021240234\n",
      "6794.904052734375\n",
      "6794.904052734375\n",
      "7490.720031738281\n",
      "7490.720031738281\n",
      "8186.929016113281\n",
      "8559.632995605469\n",
      "7410.283966064453\n",
      "7763.272979736328\n",
      "8939.976989746094\n",
      "9761.039978027344\n",
      "9086.179992675781\n",
      "9805.567016601562\n",
      "10499.715026855469\n",
      "11265.549011230469\n",
      "12039.385009765625\n",
      "12802.226989746094\n",
      "12065.203979492188\n",
      "12818.795959472656\n",
      "13536.052978515625\n",
      "12779.320007324219\n",
      "13551.960998535156\n",
      "12667.517028808594\n",
      "13630.236999511719\n",
      "14611.158996582031\n",
      "14611.158996582031\n",
      "14611.158996582031\n",
      "14611.158996582031\n",
      "14611.158996582031\n",
      "14611.158996582031\n",
      "14611.158996582031\n",
      "14611.158996582031\n",
      "14611.158996582031\n",
      "14611.158996582031\n",
      "14258.238983154297\n",
      "14625.811981201172\n",
      "14625.811981201172\n",
      "14625.811981201172\n",
      "14625.811981201172\n",
      "14625.811981201172\n",
      "14625.811981201172\n",
      "14625.811981201172\n",
      "14625.811981201172\n",
      "14625.811981201172\n",
      "14625.811981201172\n",
      "14625.811981201172\n",
      "14625.811981201172\n",
      "13518.74203491211\n",
      "14764.752044677734\n",
      "14389.657043457031\n",
      "14751.566040039062\n",
      "14751.566040039062\n",
      "14405.201049804688\n",
      "14755.707061767578\n",
      "14403.165069580078\n",
      "14750.541076660156\n",
      "14750.541076660156\n",
      "14405.196075439453\n",
      "14078.134063720703\n",
      "14397.910064697266\n",
      "14709.306060791016\n",
      "14709.306060791016\n",
      "14709.306060791016\n",
      "14709.306060791016\n",
      "14709.306060791016\n",
      "14709.306060791016\n",
      "14709.306060791016\n",
      "14709.306060791016\n",
      "14381.382049560547\n",
      "14697.245056152344\n",
      "14697.245056152344\n",
      "13801.874084472656\n",
      "14651.845092773438\n",
      "14651.845092773438\n",
      "14651.845092773438\n",
      "14651.845092773438\n",
      "14651.845092773438\n",
      "13782.530090332031\n",
      "14386.321105957031\n",
      "14672.51010131836\n",
      "13800.310089111328\n",
      "14657.163116455078\n",
      "14657.163116455078\n",
      "14657.163116455078\n",
      "14657.163116455078\n",
      "14657.163116455078\n",
      "14657.163116455078\n",
      "13952.567108154297\n",
      "14681.483123779297\n",
      "14681.483123779297\n",
      "14681.483123779297\n",
      "14681.483123779297\n",
      "14681.483123779297\n",
      "14681.483123779297\n",
      "14681.483123779297\n",
      "14448.077117919922\n",
      "14215.198120117188\n",
      "14463.045120239258\n",
      "14716.763122558594\n",
      "14716.763122558594\n",
      "14453.288116455078\n",
      "14687.203109741211\n",
      "14687.203109741211\n",
      "14687.203109741211\n",
      "14687.203109741211\n",
      "14687.203109741211\n",
      "14687.203109741211\n",
      "14013.656112670898\n",
      "14626.477096557617\n",
      "14626.477096557617\n",
      "14626.477096557617\n",
      "14626.477096557617\n",
      "14626.477096557617\n",
      "14406.367095947266\n",
      "14626.206100463867\n",
      "14626.206100463867\n",
      "14626.206100463867\n",
      "14240.892105102539\n",
      "14384.079086303711\n",
      "14618.904083251953\n",
      "14618.904083251953\n",
      "14618.904083251953\n",
      "14618.904083251953\n",
      "14618.904083251953\n",
      "14618.904083251953\n",
      "14087.202056884766\n",
      "14598.349060058594\n",
      "14598.349060058594\n",
      "14598.349060058594\n",
      "14598.349060058594\n",
      "14598.349060058594\n",
      "14598.349060058594\n",
      "14598.349060058594\n",
      "13695.599090576172\n",
      "14679.427124023438\n",
      "14679.427124023438\n",
      "14679.427124023438\n",
      "14679.427124023438\n",
      "14679.427124023438\n",
      "14403.166137695312\n",
      "14677.520141601562\n",
      "14677.520141601562\n",
      "13990.371154785156\n",
      "13990.371154785156\n",
      "14775.995178222656\n",
      "14490.658172607422\n",
      "14772.543182373047\n",
      "14772.543182373047\n",
      "14772.543182373047\n",
      "14772.543182373047\n",
      "14772.543182373047\n",
      "14772.543182373047\n",
      "13824.746215820312\n",
      "14818.267181396484\n",
      "14550.307189941406\n",
      "14550.307189941406\n",
      "14304.712188720703\n",
      "14304.712188720703\n",
      "14553.244186401367\n",
      "14800.273193359375\n",
      "14800.273193359375\n",
      "14800.273193359375\n",
      "14800.273193359375\n",
      "14800.273193359375\n",
      "14198.518188476562\n",
      "14785.252197265625\n",
      "14785.252197265625\n",
      "14785.252197265625\n",
      "14269.216186523438\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_62538/725657504.py:28: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  outputs.append(self.softmax(self.logits_layer[i](x)))\n"
     ]
    }
   ],
   "source": [
    "state = env.reset()\n",
    "\n",
    "l = []\n",
    "\n",
    "for time in range(200):\n",
    "      action = agent.act(state)\n",
    "      next_state, reward, done = env.step(action)\n",
    "      state = next_state\n",
    "      l.append(state[0])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10000.0"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.balance + env.current_price * env.position"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
