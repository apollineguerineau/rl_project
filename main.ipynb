{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trading agent demo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from modules.data_loader import DataLoader\n",
    "from modules.single_asset_env import SingleAssetEnv\n",
    "from modules.q_network import Q_network\n",
    "from modules.memory import Memory\n",
    "from modules.trading_agent import TradingAgent\n",
    "\n",
    "import torch\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_ACTIONS = 3 # Buy, Sell, Hold\n",
    "LEN_HISTORY = 30 # consider last week to predict next value\n",
    "STATES_DIM = LEN_HISTORY # history + predicted value\n",
    "\n",
    "# Q network params\n",
    "INPUT_DIM = STATES_DIM\n",
    "HIDDEN_DIM = 64\n",
    "OUTPUT_DIM = NUM_ACTIONS\n",
    "\n",
    "LEARNING_RATE = 1e-3\n",
    "\n",
    "NUM_EPOCHS = 20\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "MEMORY_SIZE = 200\n",
    "\n",
    "GAMMA = 0.97\n",
    "\n",
    "EPSILON = 1.0\n",
    "EPSILON_DECREASE = 1e-3\n",
    "EPSILON_MIN = 0.1\n",
    "START_REDUCE_EPSILON = 200\n",
    "\n",
    "TRAIN_FREQ = 10\n",
    "UPDATE_Q_FREQ = 20\n",
    "SHOW_LOG_FREQ = 5\n",
    "\n",
    "DEVICE = 'cpu'\n",
    "SEED = 123"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "DataLoader.__init__() missing 1 required positional argument: 'end_date'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m dataloader \u001b[39m=\u001b[39m DataLoader(\u001b[39m'\u001b[39;49m\u001b[39m2020-01-01\u001b[39;49m\u001b[39m'\u001b[39;49m, \u001b[39m'\u001b[39;49m\u001b[39m1d\u001b[39;49m\u001b[39m'\u001b[39;49m, \u001b[39m'\u001b[39;49m\u001b[39m2023-05-01\u001b[39;49m\u001b[39m'\u001b[39;49m)\n\u001b[1;32m      3\u001b[0m assets \u001b[39m=\u001b[39m [\u001b[39m\"\u001b[39m\u001b[39mBTC-USD\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mETH-USD\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mBNB-USD\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[1;32m      5\u001b[0m trains \u001b[39m=\u001b[39m []\n",
      "\u001b[0;31mTypeError\u001b[0m: DataLoader.__init__() missing 1 required positional argument: 'end_date'"
     ]
    }
   ],
   "source": [
    "dataloader = DataLoader('2020-01-01', '1d', '2023-05-01')\n",
    "\n",
    "assets = [\"BTC-USD\", \"ETH-USD\", \"BNB-USD\"]\n",
    "\n",
    "trains = []\n",
    "tests = []\n",
    "\n",
    "for asset in assets:\n",
    "    train, test = dataloader.load(asset)\n",
    "\n",
    "    trains.append(train)\n",
    "    tests.append(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialisations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_envs = [SingleAssetEnv(train) for train in trains]\n",
    "\n",
    "agent = TradingAgent(STATES_DIM, NUM_ACTIONS, assets, seed=SEED)\n",
    "\n",
    "scores = {key: [] for key in assets}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  0 | Scores = {'BTC-USD': 45170500.90136719, 'ETH-USD': 362313478.77067566, 'BNB-USD': 4412661.909983635}\n",
      "Epoch  1 | Scores = {'BTC-USD': 28529279.37060547, 'ETH-USD': 509770981.7358246, 'BNB-USD': 52754580.03080559}\n",
      "Epoch  2 | Scores = {'BTC-USD': 25198379.451660156, 'ETH-USD': 509770981.7358246, 'BNB-USD': 52854480.03080559}\n",
      "Epoch  3 | Scores = {'BTC-USD': 370675.8310546875, 'ETH-USD': 509770981.7358246, 'BNB-USD': 52754580.03080559}\n",
      "Epoch  4 | Scores = {'BTC-USD': 27912296.603027344, 'ETH-USD': 509692651.7122345, 'BNB-USD': 52754580.03080559}\n",
      "Epoch  5 | Scores = {'BTC-USD': 2244411.0854492188, 'ETH-USD': 512289444.0662689, 'BNB-USD': 52754580.03080559}\n",
      "Epoch  6 | Scores = {'BTC-USD': 59382050.635253906, 'ETH-USD': 509770981.7358246, 'BNB-USD': 52754580.03080559}\n",
      "Epoch  7 | Scores = {'BTC-USD': 215876309.0234375, 'ETH-USD': 285439274.13645935, 'BNB-USD': 52754580.03080559}\n",
      "Epoch  8 | Scores = {'BTC-USD': -421400.0, 'ETH-USD': 509861396.1873627, 'BNB-USD': 71933259.18829155}\n",
      "Epoch  9 | Scores = {'BTC-USD': -1018270.23046875, 'ETH-USD': 509770981.7358246, 'BNB-USD': 52754580.03080559}\n",
      "Epoch 10 | Scores = {'BTC-USD': 101534212.48144531, 'ETH-USD': 509770981.7358246, 'BNB-USD': 52754580.03080559}\n",
      "Epoch 11 | Scores = {'BTC-USD': 46328566.49609375, 'ETH-USD': 509770981.7358246, 'BNB-USD': 52598970.31984043}\n",
      "Epoch 12 | Scores = {'BTC-USD': 13088192.387695312, 'ETH-USD': 509770981.7358246, 'BNB-USD': 52754580.03080559}\n",
      "Epoch 13 | Scores = {'BTC-USD': 28955524.270507812, 'ETH-USD': 509770981.7358246, 'BNB-USD': 52715705.840940475}\n",
      "Epoch 14 | Scores = {'BTC-USD': 126916083.66552734, 'ETH-USD': 509770981.7358246, 'BNB-USD': 52754580.03080559}\n",
      "Epoch 15 | Scores = {'BTC-USD': 12819451.850585938, 'ETH-USD': 546853939.639389, 'BNB-USD': 52754580.03080559}\n",
      "Epoch 16 | Scores = {'BTC-USD': 32925724.516601562, 'ETH-USD': 509770981.7358246, 'BNB-USD': 52754580.03080559}\n",
      "Epoch 17 | Scores = {'BTC-USD': 21726138.86279297, 'ETH-USD': 312022191.4931488, 'BNB-USD': 52754580.03080559}\n",
      "Epoch 18 | Scores = {'BTC-USD': 46076765.81201172, 'ETH-USD': 507775868.76862335, 'BNB-USD': 52754580.03080559}\n",
      "Epoch 19 | Scores = {'BTC-USD': 100441597.85839844, 'ETH-USD': 509770981.7358246, 'BNB-USD': 52754580.03080559}\n",
      "Training done!\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(NUM_EPOCHS):\n",
    "\n",
    "    # intialise score for the epoch\n",
    "    score = {key: 0 for key in assets}\n",
    "    step_count = 1\n",
    "\n",
    "    for asset, env in zip(assets, train_envs):\n",
    "\n",
    "        # reset the environment before each epoch + get initial state\n",
    "        state = env.reset()\n",
    "\n",
    "        while True:\n",
    "\n",
    "            # find epsilon greedy action from state\n",
    "            action = agent.act(asset, state, 1/step_count) # epsilon = 1/t\n",
    "\n",
    "            # perform step in the environment and get completing info\n",
    "            next_state, reward, done = env.step(action)\n",
    "\n",
    "            agent.step(asset, state, action, reward, next_state, done)\n",
    "\n",
    "            # prepare for next iteration\n",
    "            step_count += 1\n",
    "            state = next_state\n",
    "\n",
    "            score[asset] += reward\n",
    "\n",
    "            if done:\n",
    "                break\n",
    "\n",
    "    # compute info about the epoch\n",
    "    for key in scores.keys():\n",
    "        scores[key].append(score[key])\n",
    "\n",
    "    print(f\"Epoch {epoch:2} | Scores = {score}\")\n",
    "\n",
    "print(\"Training done!\")\n",
    "\n",
    "# save Q_network model weights\n",
    "agent.save_models(\"weights\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing the agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial capital : 100000$ for each asset\n",
      "Balance : BTC-USD: 222693.484375\n",
      "Balance : ETH-USD: 190286.8624267578\n",
      "Balance : BNB-USD: 208606.99908447266\n",
      "---------------------------\n",
      "Total profit made: 321587.34588623047\n"
     ]
    }
   ],
   "source": [
    "test_envs = [SingleAssetEnv(test) for test in tests]\n",
    "\n",
    "print('Initial capital : 100000$ for each asset')\n",
    "total_profit = 0\n",
    "for asset, env in zip(assets, test_envs):\n",
    "\n",
    "    state = env.reset()\n",
    "\n",
    "    test_actions = []\n",
    "    test_rewards = []\n",
    "\n",
    "    for _ in range(len(env.data)-1):\n",
    "        \n",
    "        action = agent.qnets[agent.map_assets[asset]](torch.from_numpy(np.array(state, dtype=np.float32).reshape(1, -1)))\n",
    "        action = np.argmax(action.data)\n",
    "\n",
    "        test_actions.append(action.item())\n",
    "                \n",
    "        next_state, reward, done = env.step(action.numpy())\n",
    "        test_rewards.append(reward)\n",
    "\n",
    "        state = next_state\n",
    "    \n",
    "    # \n",
    "    final_running_balance = (env.data.iloc[env.t]['Close']*env.positions) + env.balance\n",
    "    total_profit += (env.data.iloc[env.t]['Close']*env.positions) + env.balance - env.initial_balance\n",
    "    print(f\"Balance : {asset}: {final_running_balance}\")\n",
    "\n",
    "print(\"-\"*27)\n",
    "print(f\"Total profit made: {total_profit}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## POUR TESTER APPLI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Balance : BTC-USD: 109533.7109375\n",
      "Balance : ETH-USD: 102530.52099609375\n",
      "Balance : BNB-USD: 100000.0\n",
      "---------------------------\n",
      "Total profit made: 12064.23193359375\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from modules.data_loader import DataLoader\n",
    "def test_agent(assets, test_envs, agent):\n",
    "    total_profit = 0\n",
    "    final_running_balance_dict = {}\n",
    "    for asset, env in zip(assets, test_envs):\n",
    "\n",
    "        state = env.reset()\n",
    "\n",
    "        test_actions = []\n",
    "        test_rewards = []\n",
    "\n",
    "        for _ in range(len(env.data)-1):\n",
    "            \n",
    "            action = agent.qnets[agent.map_assets[asset]](torch.from_numpy(np.array(state, dtype=np.float32).reshape(1, -1)))\n",
    "            action = np.argmax(action.data)\n",
    "\n",
    "            test_actions.append(action.item())\n",
    "                    \n",
    "            next_state, reward, done = env.step(action.numpy())\n",
    "            test_rewards.append(reward)\n",
    "\n",
    "            state = next_state\n",
    "    \n",
    "        final_running_balance = (env.data.iloc[env.t]['Close']*env.positions) + env.balance\n",
    "        final_running_balance_dict[asset] = final_running_balance\n",
    "        total_profit += (env.data.iloc[env.t]['Close']*env.positions) + env.balance - env.initial_balance\n",
    "        print(f\"Balance : {asset}: {final_running_balance}\")\n",
    "\n",
    "    print(\"-\"*27)\n",
    "    print(f\"Total profit made: {total_profit}\")\n",
    "\n",
    "    return final_running_balance_dict\n",
    "\n",
    "def train_agent(assets, train_envs, agent, NUM_EPOCHS):\n",
    "    scores = {key: [] for key in assets}\n",
    "    for epoch in range(NUM_EPOCHS):\n",
    "\n",
    "        # intialise score for the epoch\n",
    "        score = {key: 0 for key in assets}\n",
    "        step_count = 1\n",
    "\n",
    "        for asset, env in zip(assets, train_envs):\n",
    "\n",
    "            # reset the environment before each epoch + get initial state\n",
    "            state = env.reset()\n",
    "\n",
    "            while True:\n",
    "\n",
    "                # find epsilon greedy action from state\n",
    "                action = agent.act(asset, state, 1/step_count) # epsilon = 1/t\n",
    "\n",
    "                # perform step in the environment and get completing info\n",
    "                next_state, reward, done = env.step(action)\n",
    "\n",
    "                agent.step(asset, state, action, reward, next_state, done)\n",
    "\n",
    "                # prepare for next iteration\n",
    "                step_count += 1\n",
    "                state = next_state\n",
    "\n",
    "                score[asset] += reward\n",
    "\n",
    "                if done:\n",
    "                    break\n",
    "\n",
    "        # compute info about the epoch\n",
    "        for key in scores.keys():\n",
    "            scores[key].append(score[key])\n",
    "\n",
    "        print(f\"Epoch {epoch:2} | Scores = {score}\")\n",
    "\n",
    "    print(\"Training done!\")\n",
    "\n",
    "    # save Q_network model weights\n",
    "    agent.save_models(\"weights\")\n",
    "\n",
    "\n",
    "asset_name_test = \"BTC-USD\"\n",
    "model_path = \"weights/trained_agent_model_\"+asset_name_test+\".pth\"\n",
    "assets = [\"BTC-USD\", \"ETH-USD\", \"BNB-USD\"]\n",
    "\n",
    "## Load Training and Testing Dataset\n",
    "date_split = '2023-01-01'\n",
    "start_date = '2020-01-01'\n",
    "end_date =  '2024-01-01'\n",
    "dataloader = DataLoader(start_date, '1d', date_split, end_date)\n",
    "\n",
    "\n",
    "trains = []\n",
    "tests = []\n",
    "\n",
    "for asset in assets:\n",
    "    train, test = dataloader.load(asset)\n",
    "\n",
    "    trains.append(train)\n",
    "    tests.append(test)\n",
    "\n",
    "# Environment and Agent Initiation\n",
    "train_envs = [SingleAssetEnv(train) for train in trains]\n",
    "test_envs = [SingleAssetEnv(test) for test in tests]\n",
    "agent = TradingAgent(STATES_DIM, NUM_ACTIONS, assets, seed=SEED)\n",
    "\n",
    "if os.path.exists(model_path) :\n",
    "    ## Agent\n",
    "    model_path = \"weights/trained_agent_model_\"+asset_name_test+\".pth\"\n",
    "    # Load the state dict\n",
    "    state_dict = torch.load(model_path)\n",
    "\n",
    "    # Set the loaded state dict to the Q network of the corresponding asset\n",
    "    agent.qnets[agent.map_assets[asset]].load_state_dict(state_dict)\n",
    "\n",
    "    final_running_balance_dict = test_agent(assets, test_envs, agent)\n",
    "\n",
    "else:  # TRAINING MODEL IN CASE IT IS NOT TRAINED YET\n",
    "    agent = TradingAgent(STATES_DIM, NUM_ACTIONS, assets, seed=SEED)    \n",
    "    train_agent(assets, train_envs, agent, NUM_EPOCHS)\n",
    "    final_running_balance_dict = test_agent(assets, test_envs, agent)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.12 ('myenv')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "8c0acf22ca7497d002e5e6e252fe169a1f9bf4afdf57db37b4725403d5188d16"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
